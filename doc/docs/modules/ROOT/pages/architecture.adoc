[#architecture]
= Architecture Guidance for Implementing

[abstract]
--
This chapter describes provides tips and tricks on how to get the best performance.
--

== Overview

No matter which direction you're going, whether sending data from Spark to Neo4j or pulling data out of Neo4j,
the two systems are based on completely different organizing data models:

* Spark is oriented around tabular *DataFrames*
* Neo4j is oriented around *graphs* (nodes, relationships, paths)

The use of this connector can be thought of as a "Transform" step and a "Load" step, regardless of
which direction the data is moving.  The "Transform" step is concerned with how to move data between
graphs and tables.  And the "Load" step is concerned with moving masses of data.

== Graph Transforms

When taking any complex set of dataframes and preparing it for load into Neo4j, you have basically two options:
* Normalized Loading
* Cypher Destructuring

This section will describe both, and provide information on pros and cons from a performance and complexity perspective.

[NOTE]
**Where possible, use the normalized loading approach for best performance & maintainability**

=== Normalized Loading

Suppose we want to load a single dataframe called `purchases` into Neo4j with the following contents:

```csv
product_id,product,customer_id,customer,quantity
1,Socks,10,David,2
2,Pants,11,Andrea,1
```

This data represents as simple `(:Customer)-[:BOUGHT]->(:Product)` "graph shape".

The normalized loading approach requires that you create a number of different dataframes; one for each node type
and relationship type in your desired graph.  For example, in this case, we might create 3 dataframes:

* `val products = spark.sql("SELECT product_id, product FROM purchases")`
* `val customers = spark.sql("SELECT customer_id, customer FROM purchases")`
* `val bought = spark.sql("SELECT product_id, customer_id, quantity FROM purchases")`

Once these simple data frames represent a normalized view of "tables for labels" (that is, one DataFrame/table per node label or relationship) - then the existing utilities provided by the connector for writing nodes & relationships can be used with
no additional cypher needed.  Additionally -- if these frames are made unique by identifier, then the data is already
prepared for maximum parallelism.  (See paralellism notes in sections below).

==== Pros

* Shifts most of the data transform work to spark itself (in terms of splitting, uniquing, partitioning the data).  Any
data transform/cleanup work should be done in spark if possible
* Makes for easy to follow code; in the end, the write of each dataframe to Neo4j is quite simple, and requires mostly just
a label and a key
* Allows for parallelism (discussed in sections below)

==== Cons

* More SQL work before data is loaded into Neo4j
* Requires identifying graph schema before beginning, as opposed to loading data into Neo4j and using cypher to manipulate it
afterwards

=== Cypher Destructuring

Cypher destructuring is the process of using a single cypher statement to process a complex record into a finished graph
pattern.  Let's look again at the data example:

```csv
product_id,product,customer_id,customer,quantity
1,Socks,10,David,2
2,Pants,11,Andrea,1
```

To store this in Neo4j, we might use a cypher query like this:

```cypher
MERGE (p:Product { id: event.product_id })
  ON CREATE SET p.name = event.product
WITH p
MERGE (c:Customer { id: event.customer_id })
  ON CREATE SET c.name = event.customer
MERGE (c)-[:BOUGHT { quantity: event.quantity }]->(p);
```

Notice that in this case the entire job can be done by a single cypher statement.  As data frames get complex,
these cypher statements too can get quite complex.  

==== Pros

* Extremely flexible: you can do anything that Cypher provides for
* Easy for Neo4j pros to get started with.

==== Cons

* Tends to shift transform work to Neo4j, which is not a good idea as it does not have the same infrastructure to support that as Spark.
* Tends to create heavy locking behavior, which will harm parallelism and possibly performance
* Encourages you to embed schema information in a cypher query rather than use spark utilities

== Graph Transforms:  General Principles

* Wherever possible, perform data quality fixes prior to loading into Neo4j; this includes dropping missing records, changing datatypes of properties, and so on.
* Because Spark excels at parallel computation, any non-graph heavy computation should be done in the spark layer, rather than
in Cypher on Neo4j
* Size your Neo4j instance appropriately before using aggressive parallelism or large batch sizes
* Experiment with larger batch sizes (ensuring that batches stay within Neo4j's configured heap memory).  In general,
the larger the batches, the faster the overall throughput to Neo4j.

== Transforming from Graphs Back to DataFrames

[NOTE]
**In general, always have an explicit RETURN statement and destructure your results**

A common pattern will be to write a complex cypher statement, perhaps one that traverses many relationships, to return
a dataset to spark. Because spark does not understand graph primitives, there are not many useful ways that a raw node,
relationship, or path can be represented in spark.  As a result we recommend you do not return those types from Cypher
to Spark, instead focusing on concrete property values and function results, which can be represented as simple types
in spark.

For example, this query would result in an awkward dataframe that would be hard to manipulate:

```cypher
MATCH path=(p:Person { name: "Andrea" })-[r:KNOWS*]->(o:Person)
RETURN path;
```

A better query which will result in a cleaner DataFrame is as follows:

```cypher
MATCH path=(p:Person { name: "Andrea" })-[r:KNOWS*]->(o:Person)
RETURN length(path) as pathLength, p.name as p1Name, o.name as p2Name
```

== Parallelism

Spark is fundamentally about partitioning and paralleism; the go-to technique is to split a batch of
data into partitions for each machine to work on in parallel.   In Neo4j, parallelism works very differently, which
we will describe in this chapter.

=== Write Parallelism in Neo4j

[NOTE]
**For most writes to Neo4j, it is strongly recommended to repartition your dataframe to 1 partition only**

When writing nodes & relationships in Neo4j:

* Writing a relationship locks both nodes
* Writing a node locks the node

Additionally, in Neo4j's causal cluster model, only the cluster leader may write data.  This means that
because writes scale vertically in Neo4j, the practical paralleism is limited to the number of cores on the leader.

The reason a single partition for writes is recommended is because it eliminates lock contention between writes.  Suppose
one partition is writing:

```
(:Person { name: "Michael" })-[:KNOWS]->(:Person { name: "Andrea" })
```

while another partition is writing:

```
(:Person { name: "Andrea" })-[:KNOWS]->(:Person { name: "Davide" })
```

The relationship write will lock the "Andrea" node - and these writes cannot continue in parallel in any case.  As
a result, you may not gain performance by parallelizing more, if threads have to wait for each other's locks.  In 
extreme cases with too much parallelism, Neo4j may reject the writes with lock contention errors.

=== Dataset Partitioning

[NOTE]
**You can use as many partitions as there are cores in the Neo4j server, if you have properly partitioned your data to avoid Neo4j locks**

There is an exception to the "1 partition" rule above; if your data writes are partitioned ahead of time to avoid locks, you 
can generally do as many write threads to Neo4j as there are cores in the server.   Suppose we want to write a long list of `:Person` nodes, and we know they are distinct by the person `id`.  We might stream those into Neo4j in 4 different partitions, as there will not be any lock contention.

== Schema Considerations

Neo4j does not have a fixed schema; individual properties can contain multiple differently-typed values.  Spark
on the other hand will tend to expect a fixed schema.  For this reason, the connector contains a number of schema 
inference techniques that help ease this mapping.  Paying close attention to how these features work can help 
explain different scenarios.

The two core techniques are:
* `CALL apoc.meta.(node|rel)TypeProperties()`
* Sampling

=== APOC

If your Neo4j installation has APOC installed, this approach will be used by default.  These stored procedures within APOC allow inspection of the
metadata in your graph, and provide information such as the type of properties, and the universe of possible properties attached to a given node label.

You may try these calls yourself on your Neo4j database if you wish: simply execute:

```cypher
CALL apoc.meta.nodeTypeProperties()
```

And inspect the results.  These results are how the Neo4j Connector for Apache Spark represents the metadata of nodes & relationships read into DataFrames.

=== Automatic Sampling

In some installations and environments, the key APOC calls above will not be available.  In these cases, the connector will automatically sample the first few
records and infer the right data type from the examples that it sees.

[NOTE]
**Automatic sampling may be error prone, and may produce incorrect results, particularly in cases where a single Neo4j property exists with several different data types.  Consistent typing of properties is strongly recommended**
