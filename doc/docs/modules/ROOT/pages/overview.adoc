
= Project Overview

*Neo4j Spark Connector* allows you to read from and write to Neo4j databases.

It's fairly easy to use, although it can be highly customized.

You can read your Neo4j database and have the data available as Spark DataFrame.

.Read all the nodes with of type Person
[source,scala]
----
import org.apache.spark.sql.{SaveMode, SparkSession}

val spark = SparkSession.builder().getOrCreate()

val df = spark.read.format("org.neo4j.spark.DataSource")
  .option("url", "bolt://localhost:7687")
  .option("labels", "Person")
  .load()
----

Similarly, is possible to write your own DataFrame to Neo4j:

.Write the DataFrame to nodes of type Person
[source,scala]
----
import org.apache.spark.sql.{SaveMode, SparkSession}

val spark = SparkSession.builder().getOrCreate()
import spark.implicits._

val df = Seq(
  ("John Doe"),
  ("Jane Doe")
).toDF("name")

df.write.format("org.neo4j.spark.DataSource")
  .mode(SaveMode.ErrorIfExists)
  .option("url", "bolt://localhost:7687")
  .option("labels", ":Person")
  .save()
----

Visit the link:reading[Reading] and link:writing[Writing] sections for advanced usage.
