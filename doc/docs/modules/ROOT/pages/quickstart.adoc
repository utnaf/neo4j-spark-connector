
= Quick Start

[abstract]
--
Get started fast for common scenarios, using Neo4j Streams plugin or Kafka Connect plugin
--

== Neo4j Spark Connector Plugin

=== Overview

Neo4j Spark Connector allows Spark to read from and write to Neo4j databases.

=== Getting Started

Reading all the nodes of type `Person` from your local Neo4j instance is as simple as this:

```scala
val df = spark.read.format("org.neo4j.spark.DataSource")
        .option("url", "bolt://localhost:7687")
        .option("labels", "Person")
        .load()
```

=== Neo4j Options

Spark method `option` is used to configure the Neo4j Connector.

```scala
val df = spark.read.format("org.neo4j.spark.DataSource")
        .option("url", "bolt://localhost:7687")
        .option("authentication.type", "basic")
        .option("authentication.basic.username", "myuser")
        .option("authentication.basic.password", "neo4jpassword")
        .option("labels", "Person")
        .load()
```

.Most Common Needed Configuration Settings
|===
|Setting Name |Description |Default Value |Required 

4+|*Driver Options*

|`url`
|The url of the Neo4j instance to connect to
|_(none)_
|Yes

|`authentication.type`
|The authentication method to be used: `none`, `basic`, `kerberos`, `custom`. More info https://neo4j.com/docs/driver-manual/4.1/client-applications/#driver-authentication[here]
|`basic`
|No

|`authentication.basic.username`
|Username to use for basic authentication type
|``
|No

|`authentication.basic.password`
|Username to use for basic authentication type
|``
|No

|`authentication.kerberos.ticket`
|Kerberos Auth Ticket
|``
|No

|`authentication.custom.principal`
|This used to identify who this token represents
|``
|No

|`authentication.custom.credentials`
|This are the credentials authenticating the principal
|``
|No

|`authentication.custom.realm`
|This is the "realm:"", specifying the authentication provider
|``
|No

|`encryption.enabled`
|Specify if encryption should be enabled. This setting is totally ignored if you use a URI scheme with +s or +ssc
|`false`
|No

|`encryption.trust.strategy`
|Set cerificate trust strategy, is ignored in case the connection URI uses `+s` or `+ssc` as suffix.
|`TRUST_SYSTEM_CA_SIGNED_CERTIFICATES`
|No

|`encryption.ca.certificate.path`
|Set certificate path for `TRUST_CUSTOM_CA_SIGNED_CERTIFICATES` trust strategy
|``
|No

|`connection.max.lifetime.msecs`
|Connection lifetime in milliseconds
|(Neo4j Driver default)
|No

|`connection.liveness.timeout.msec`
|Liveness check timeout
|(Neo4j Driver default)
|No

|`connection.acquisition.timeout.msecs`
|Connection acquisition timeout in milliseconds
|(Neo4j Driver default)
|No

|`connection.timeout.msecs`
|Connection timeout in milliseconds
|(Neo4j Driver default)
|No

4+|*Session Options*

|`database`
|Database name to connect to. As the driver allows to define the database in the URL,
in case you set this option will have the priority compared to the one defined in the URL
|``
|No

|`access.mode`
|Used only while you're pulling data from Neo4j. In case is `read` the connector, in a cluster environment
will route the requests to the followers, otherwise to the leader.
|`read`
|No

4+|*Data Read Options*

|`query`
|Specify a query to read the data
|``
|Yes^*^

|`labels`
|List of labels separated by `:`. First label is the primary label
|``
|Yes^*^

|`relationship`
|List of labels separated by `:` (first label is the primary label)
|``
|Yes^*^

|`relationship.nodes.map`
|If true return `source` and `target` nodes as Map<String, String>, otherwise we flatten the properties by returning
every single node property as column prefixed by `source` or `target`
|`true`
|No

|`relationship.source.labels`
|List of source node Labels separated by `:`
|``
|Yes^*^

|`relationship.target.labels`
|List of target node Labels separated by `:`
|``
|Yes^*^

|`schema.flatten.limit`
|Number of records to be used to create the Schema (only if APOC are not installed)
|`10`
|No

|`schema.strategy`
|Strategy used by the connector in order to compute the Schema definition for the Dataset. Values `string` and
`sample`. If `string` coerces all the properties to String otherwise it will try to sample the Neo4j's dataset.
|`sample`
|No

|`pushdown.filters.enabled`
|Enable or disable the Push Down Filters support
|`true`
|No

4+|Data Write Options

|`batch.size`
|Number of records to be passed to the transaction
|`5000`
|No

|`relationship.access.mode`
|<<node-keys,Docs here>>
|``
|Yes for <<keys-strategy,KEYS>> strategy

|`relationship.source.labels`
|List of source node Labels separated by `:`
|``
|Yes^*^

|`relationship.source.node.keys`
|<<node-keys,Docs here>>
|``
|Yes for <<keys-strategy,KEYS>> strategy

|`relationship.source.access.mode`
|<<node-access-modes,Node Access Mode>>
|`Match`
|No

|`relationship.target.labels`
|List of target node Labels separated by `:`
|``
|Yes^*^

|`relationship.target.access.mode`
|<<node-access-modes,Node Access Mode>>
|`Match`
|No

|`relationship.target.node.keys`
|<<node-keys,Docs here>>
|``
|Yes for <<keys-strategy,KEYS>> strategy

|===

^*^ Just one of the options can be specified.

== Read Data

Reading data from a Neo4j Database can be done in 3 ways:

 * with a Cypher query
 * with a set of node Labels 
 * by specifying a relationship

=== Considerations on the schema

Spark works with data in a tabular fixed schema. To accomplish this Neo4j Connector has a schema infer system that creates the schema based on the data requested for the read. Each read data method has is own strategy to create it, that will be explained it each section.

TK list of supported data types

=== Consideration on the filters

The Neo4j Spark Connector implements the SupportPushDownFilters interface, that allows you to push the Spark filters down to the Neo4j layer. In this way the data that Spark will receive will be already filtered by Neo4j.

You can manually disable the Push Down Filters support using the `pushdown.filters.enabled` option and set it to `false` (default is `true`).

If you use use the filter function more than once, like in this example:
```scala
df.where("name = 'John Doe'").where("age = 32")
```
The conditions will be automatically joined with an `AND` operator.

[NOTE]
When using `relationship.node.map = true` or `query` the PushDownFilters support is not active, thus the filters will be applied by Spark and not by Neo4j.

==== How we extract the schema

As Neo4j has a schema-less approach and Spark needs a Schema in order to create a Dataset,
we use several approaches in order to sample the dataset into Neo4j and compute the schema for Spark's Dataset.

===== Extract schema for Nodes

In case you're extracting nodes from Neo4j we try as first step to invoke the `apoc.meta.nodeTypeProperties` procedure,
in case the procedure is not installed we'll execute the following Cypher query:

```cypher
MATCH (n:<labels>)
RETURN n
ORDER BY rand()
LIMIT <limit>
```

Where `<labels>` is the list of labels provided via `.option("labels", ":MyLabel:MyOtherLabel")` and `<limit>` is the
value provided via `.option("schema.flatten.limit", "100")`

===== Extract schema for Relationships

In case you're extracting nodes from Neo4j we try as first step to invoke the `apoc.meta.relTypeProperties` procedure,
in case the procedure is not installed we'll execute the following Cypher query:

```cypher
MATCH (source:<source_labels>)-[rel:<relationship>]->(target:<target_labels>)
RETURN rel
ORDER BY rand()
LIMIT <limit>
```

Where:

 * `<source_labels>` is the list of labels provided via `.option("relationship.source.labels", ":MyLabel:MyOtherLabel")`
 * `<target_labels>` is the list of labels provided via `.option("relationship.target.labels", ":MyLabel:MyOtherLabel")`
 * `<relationship>` is the list of labels provided via `.option("relationship", "MY_RELATIONSHIP")`
 * `<limit>` is the value provided via `.option("schema.flatten.limit", "100")`

==== Complex Data Types

Spark doesn't support all Neo4j data types (ie: Point, Time, Duration). Such types are transformed into Struct types containing all the useful data.

|===

|Type |Struct 

|`Duration`
a|
----
Struct(Array(
    ("type", DataTypes.StringType, false),
    ("months", DataTypes.LongType, false),
    ("days", DataTypes.LongType, false),
    ("seconds", DataTypes.LongType, false),
    ("nanoseconds", DataTypes.IntegerType, false),
    ("value", DataTypes.StringType, false)
  ))
----

|`Point`
a|
----
Struct(Array(
    ("type", DataTypes.StringType, false),
    ("srid", DataTypes.IntegerType, false),
    ("x", DataTypes.DoubleType, false),
    ("y", DataTypes.DoubleType, false),
    ("z", DataTypes.DoubleType, true),
  ))
----

|`Time`
a|
----
Struct(Array(
    ("type", DataTypes.StringType, false),
    ("value", DataTypes.StringType, false)
  ))
----

|=== 

=== Read data by Node Labels

You can both specify a single label, like this example
```scala
val df = spark.read.format("org.neo4j.spark.DataSource")
        .option("url", "bolt://localhost:7687")
        .option("labels", "Person")
        .load()

df.show()
```

Multiple labels can be specified, separated by `:`
```scala
val df = spark.read.format("org.neo4j.spark.DataSource")
        .option("url", "bolt://localhost:7687")
        .option("labels", "Person:Customer:Admin")
        .load()

df.show()
```

When reading data with this method, the Dataframe will contain all the fields contained in the nodes, plus 2 additional columns.

 * `<id>` the internal Neo4j id
 * `<labels>` a list of labels for that node

==== Schema

If APOC are available, the schema will be created with `apoc.meta.nodeTypeProperties`.
Otherwise the first 10 (or any number specified by the `schema.flatten.limit` option) results will be flattened and the schema will be create from those properties.

===== Example

```
CREATE (p1:Person {age: 31, name: 'Jane Doe'}),
    (p2:Person {name: 'John Doe', age: 33, location: null}),
    (p3:Person {age: 25, location: point({latitude: -37.659560, longitude: -68.178060})})
```

Will create this schema

|===
|Field |Type 

|<id>|Int

|<labels>|String[]

|age|Int

|name|String

|location|Point

|===

=== Read data by Relationship Type

You can specify a Cypher Path in this way:
```scala
val df = spark.read.format("org.neo4j.spark.DataSource")
        .option("url", "bolt://localhost:7687")
        .option("relationship", "BOUGHT")
        .option("relationship.source.labels", "Person")
        .option("relationship.target.labels", "Product")
        .load()

df.show()
```

This will create a Cypher Query as it follows:

```cypher
MATCH (source:Person)-[rel:BOUGHT]->(target:Product)
RETURN source, rel, target
```

When reading data with this method, the Dataframe will contain all the fields contained in the relationship, plus:

* `<id>` the internal Neo4j id
* `<relationshipType>` the relationship type

and depending on the value of `relationship.node.map` option, if `true`:

* `source` the Map<String, String> of source node
* `target` the Map<String, String> of target node

otherwise if `false`:

[[bookmark-rel-read-schema]]
* `<sourceId>` the internal Neo4j id of source node
* `<sourceLabels>` a list of labels for source node
* `<targetId>` the internal Neo4j id of target node
* `<targetLabels>` a list of labels for target node
* `source.[property name]` a list of properties of the source node
* `target.[property name]` a list of properties of the target node
* `rel.[property name]` a list of properties of the relationship

==== Filter

You can use Spark to filter properties of the relationship, the source node, or the target node. Just use the correct prefix:

If `relationship.node.map` is set to **false**

* ``\`source.[property]` `` for the source node properties
* ``\`rel.[property]` `` for the relation property
* ``\`target.[property]` `` for the target node property

If `relationship.node.map` is set to **true**

* ``\`<source>`.\`[property]` `` for the source node map properties
* ``\`<rel>`.\`[property]` `` for the relation map property
* ``\`<target>`.\`[property]` `` for the target node map property

in this case, all the map values will be strings, so the filter value must be a string too.

```scala
val df = spark.read.format("org.neo4j.spark.DataSource")
      .option("relationship.node.map", false)
      .option("relationship", "BOUGHT")
      .option("relationship.source.labels", "Person")
      .option("relationship.target.labels", "Product")
      .load()

df.where("source.name = 'John Doe' AND target.price >= 33")
```

==== Schema

If APOC are available, the schema will be created with `apoc.meta.relTypeProperties`.
Otherwise the first 10 (or any number specified by the `schema.flatten.limit` option) results will be flattened and the schema will be create from those properties.

=== Read data by custom Cypher Query

You can specify a Cypher query in this way:
```scala
val df = spark.read.format("org.neo4j.spark.DataSource")
      .option("query", "MATCH (n:Person) WITH n LIMIT 2 RETURN collect(n) AS nodes")
      .load()

df.show()
```

[NOTE]
We recommend that individual property fields be returned, rather than returning  graph entity (node, relationship, and path) types.
This best maps to spark's type system and yields best results.
So instead writing this `MATCH (p:Person) RETURN p` please write this: `MATCH (p:Person) RETURN id(p) as id, p.name as name`.
If your query returns a graph entity please use the `labels` or `relationship`.

The struct of the Dataset returned by the query is influenced by the query itself, in this particular context it could happen
that the connector could not be able to sample the Schema from the query, in these particular cases we suggest trying with
the option `schema.strategy` defined as `string` as it follows:

```scala
val df = spark.read.format("org.neo4j.spark.DataSource")
      .option("query", "MATCH (n:Person) WITH n LIMIT 2 RETURN collect(n) AS nodes")
      .option("schema.strategy", "string")
      .load()

df.show()
```

This means that the struct returned by the query will be composed by strings that you can than cast via simply Spark's
transformations.

[NOTE]
Inference (`schema.strategy` = `sample`) is good when all instances of a property in neo4j are the same type,
and string followed by cast is better when property types may differ.
Remember that Neo4j does not enforce property typing, and so `person.age` could sometimes be a `long
and sometimes be a `string`.

==== Schema

If APOC are installed, schema will be created with `apoc.meta.relTypeProperties`. Otherwise the first 10 (or any number specified by the `schema.flatten.limit` option) results will be flattened and the schema will be create from those properties.

=== Write data by Relationship Type

You can write a dataframe to Neo4j by specifying source, target and relation.
There are two strategies you can use to write relationships: *NATIVE* and *KEYS*

==== Native Strategy

This strategy is useful when you have a schema that conforms with the <<bookmark-rel-read-schema,Relationship Read Schema>>

```scala
df.write
    .format("org.neo4j.spark.DataSource")
    .option("url", "bolt://localhost:7687")
    .option("relationship", "SOLD")
    .option("relationship.source.labels", ":Person")
    .option("relationship.source.write.mode", "overwrite")
    .option("relationship.target.labels", ":Product")
    .option("relationship.target.write.mode", "overwrite")
    .save()
```

You just need to specify the source node labels, the target node labels, and the relationship you want between them.

[NOTE]
The default write mode for source and target nodes is `match`. This means that the relationship will be created only if the nodes are already in your DB. Look at <<node-access-modes,here>> for more info about node write modes.

[[keys-strategy]]
==== Keys Strategy

For all the other cases you can usee the *KEYS* strategy.
Say you have a Dataframe like this:

```scala
val musicDf = Seq(
        (12, "John Bonham", "Drums"),
        (19, "John Mayer", "Guitar"),
        (32, "John Scofield", "Guitar"),
        (15, "John Butler", "Guitar")
    ).toDF("experience", "name", "instrument")
```

To write it to Neo4j is enough to speficy a list of attributes and

```scala
musicDf.write
    .format("org.neo4j.spark.DataSource")
    .option("url", "bolt://localhost:7687")
    .option("relationship", "PLAYS")
    .option("relationship.write.strategy", "keys")
    .option("relationship.source.labels", ":Musician")
    .option("relationship.source.write.mode", "overwrite")
    .option("relationship.source.node.keys", "name:name")
    .option("relationship.target.labels", ":Instrument")
    .option("relationship.target.node.keys", "instrument:name")
    .option("relationship.target.write.mode", "overwrite")
    .save()
```

[[node-keys]]
Here you must speficy which keys of your Dataframe will be writtent in the source node and in the target node.
All the remaining properties will be written in the relationship.
You can do this with a comma-separated list of `key:value` paris, where the key is the node property name to be written, and the value is the dataframe column name.

[[node-access-modes]]
===== Node Access Modes

You can specify 3 different modes to use for writing the nodes:

* `Overwrite`: will perform a `MERGE` on that node
* `ErrorIfExists`: will perform a `CREATE`
* `Match`: will perform a `Match`